# Data Lake with PowerBI
_Proyecto de fin de semestre en el cual se crear√° un DATA LAKE enfocado en la recopilaci√≥n de datos mediante websacraping, API Keys, p√°ginas que permiten realizar webscraping._

## Objetivo del apartado üöÄ

_Se recopilaran datos de la red social Twitter utilizando coordenadas y la palabra clave ‚Äòaborto‚Äô, as√≠ mismo de TikTok con  [tiktok-scraper](https://github.com/drawrowfly/tiktok-scraper/tree/develop) utilizando la misma palabra clave y websacrping  especializado para esta red social y para finalizar  utilizaremos la p√°gina web [Apify](https://apify.com/) para realizar scraping a YouTube._

### Recopilaci√≥n de datos de Twitter
_Para iniciar este proceso es necesario tener las API Keys que Twitter nos proporciona y con la ayuda de la p√°gina web [Boundingbox](https://boundingbox.klokantech.com/) conseguiremos las coordenadas de las cuidades que deseamos solicitar datos._
_KEY's_
![image](https://user-images.githubusercontent.com/75056800/156851331-11242016-3ca1-4f8d-98a9-b974f9cae7f0.png)

_Coordenadas de BouBoundingbox_
![image](https://user-images.githubusercontent.com/75056800/156851460-c3c1ee40-7678-49e6-8c3f-e753057f2862.png)

_Una vez que cumplimos estos requisitos procederemos mediante scripts de Python a extraer los datos envi√°ndole los datos conseguidos anteriormente hay que tomar en cuenta tambi√©n que se debe instalar [Tweepy](https://docs.tweepy.org/en/stable/) en nuestras librer√≠as_
![image](https://user-images.githubusercontent.com/75056800/156852001-fc99ff04-3aee-4502-8e05-7c741852a3b9.png)

_Podemos observar que iniciamos importando nuestras librer√≠as tanto de CouchDB acompa√±ado de otras librer√≠as necesarias para este paso, se creara una clase que mediante  tweepy va empezar a extraer los datos que necesitamos y los va a ir guardando en nuestra base de datos, como paso siguiente vamos a crear nuestra conexi√≥n con CouchDB utilizando los m√©todos que nos brinda para este caso, le enviamos la ruta del servidor en este caso el Localhost apuntando al puerto 5984 juto al usuario y contrase√±a registrados al momento de la instalaci√≥n._

![image](https://user-images.githubusercontent.com/75056800/156852737-14696bde-dc0b-4339-88ea-9b5d0656d286.png)

_Dentro de estas l√≠neas de c√≥digo podemos ver que se encuentra una variable que va a guardar nuestra clases que recibe como par√°metros las Keys de Twitter, una vez listos mediante el m√©todo filter podemos enviarle nuestras coordenadas seleccionadas y la palabra clave. El script empezara a buscar la extracci√≥n de los datos._

![image](https://user-images.githubusercontent.com/75056800/156856402-bfc57dd1-7056-4980-a389-620f0872abbd.png)

_Este proceso se va a ver reflejado en CouchDB despues de haberlo realizado con varias coordenadas._

![image](https://user-images.githubusercontent.com/75056800/156856502-771caa9d-6251-439d-91b9-7bf8bd7efca9.png)

_Para continuar extraeremos datos de TikTok utilizando tiktok-scraper utilizando una l√≠nea de c√≥digo que insertaremos en CDM o terminal del sistema operativo utilizado apuntando a la carpeta donde se va a guardar la informaci√≥n_

![image](https://user-images.githubusercontent.com/75056800/156857035-5cb6c188-82f2-4a30-bf46-ea43b1d23ab8.png)

_Este m√©todo de scraping nos permite a mas de conseguir la informaci√≥n en formato json csv tambi√©n noes permite descargar los videos de los usuarios utilizando -d_

![image](https://user-images.githubusercontent.com/75056800/156857262-9f9b864b-ae78-42d0-9488-6bd9fb39d711.png)

_En este caso solamente descargamos los documentos y dejamos a un lado los videos para no cargar nuestro sistema._

_Ahora vamos a visitar [Apify](https://apify.com/) que despu√©s de registrarnos nos va a permitir realizar web sacraping a YouTube_

![image](https://user-images.githubusercontent.com/75056800/156859715-4bf43284-cb87-4fb6-993e-0f40983a0e08.png)

_Como podemos observar esta p√°gina nos permite ingresar una palabra clave y el n√∫mero m√°ximo de documentos que esperamos conseguir, damio clic en iniciar y nos indicara el proceso de nuestra tarea._

![image](https://user-images.githubusercontent.com/75056800/156859888-f16dca56-b81b-4366-9f80-4f967ba6c1df.png)

_Una vez terminado el proceso nos indicara lso datos que recolectamos dentro de una archivo json o csv y otros formatos y lo descargamos.
![image](https://user-images.githubusercontent.com/75056800/156860048-7f84ab14-f979-44bd-abcd-adca6cff0275.png)


_Estos datos dentro de documentos o bases de datos tienes que ser concentrados en MongoDB para esto utilizaremos otros scripts uqe nos permitiran extraexlos de las BD de marea sistematica, para los procesos que como resultado nos entregaron archivos json o csv utilizaremos primero el SGBD MySQL Workbench._

_Utilizaremos el wizard para poder importar los documentos y eliminaremos las columnas que no deseamos para poder concentrar los datos de Tiktok._

![image](https://user-images.githubusercontent.com/75056800/156860830-0ced4c03-7bd0-409b-b9d9-f23b832d790f.png)

![image](https://user-images.githubusercontent.com/75056800/156860910-636fcf7b-8b09-4963-8dab-5e981ac848e4.png)

_Una vez seleccionadas las columnas que nos van a servir para nuestro proyecto guardamos y verificamos la carga, tenemos una BD abortos base que contiene 2 tablas con la informaci√≥n necesaria_

![image](https://user-images.githubusercontent.com/75056800/156861167-70e7b25c-2c88-4670-9407-f5d7a6b4c902.png)


_Para poder implementar el documento que obtuvimos de YouTube scraping utilizaremos XAMPP que nos da el ingreso a PhpMySQL_

![image](https://user-images.githubusercontent.com/75056800/156861521-f0975801-451e-47f4-9376-de0873a4680c.png)

_Observamos que una vez iniciado el servidor apache podremos iniciar el servicio SQL ahora importaremos nuestro archivo json utilizando el m√©todo anterior y podremos ver nuestros datos en esta SGBD _

![image](https://user-images.githubusercontent.com/75056800/156861706-a50050c4-5262-49c1-9bfa-6440457b9cc6.png)

## Importaci√≥n de datos a MongoDB

_Para este proceso utilizaremos 2 scripts que nos permitiran conectarnos tanto a CouchDB y a SQL_

_De CouchDb a MongoD_

_ Para iniciar cargaremos nuestros controladores y realizaremos nuestras conexiones con los respetivos servidores d√°ndoles los nombres necesarios en cada SGBD_

![image](https://user-images.githubusercontent.com/75056800/156862228-17074f58-c91d-421f-a8d3-b0231bcc6cff.png)

_La conexi√≥n se ha realizado con √©xito y ya tenemos un nombre de base de datos para guardar mediante un m√©todo que recorre todo el documento procederemos a pasa nuestra informaci√≥n._

![image](https://user-images.githubusercontent.com/75056800/156862551-5357998d-4512-46aa-b4bc-78c26eb06883.png)


_Como veremos en la siguiente imagen tenemos las mismas bases dedatos en CouchDB y MongoDB_

![image](https://user-images.githubusercontent.com/75056800/156862607-9a094a2d-ec16-4031-92fd-16f392189b4d.png)


_Para pasar lod datos desde MySQL y PhpMyadmin utilizaremos un script parecido pero en esta ocasi√≥n crearemos un dataframe con la ayuda de Pandas que ser√° codificado y enviado a MongoDB._

![image](https://user-images.githubusercontent.com/75056800/156862740-a4334c65-ac7b-45bc-91b9-5b04c7e596b2.png)

_En la primera parte de nuestro script creamos las conexiones tanto para SGBD SQL y par MongoDB es importante tener instalados los drivers pymongo que nos permite crear un cliente para la conexi√≥n este cliente le entregamos nuestro usuario, contrase√±a, servidor donde se ubica y la base de datos a la que apuntamos, si la conexi√≥n se realiza con √©xito nos creara un cursos que nos permitir√° ejecutar sentencias SQL._

![image](https://user-images.githubusercontent.com/75056800/156863154-a98604b6-59df-4082-8fc0-ee234d83b769.png)


_Creamos un dataframe que guardara toda la informaci√≥n que nos extraiga la consulta SQL en MongoDb se creara la base de datos de llegada con su colecci√≥n, creamos una funci√≥n que nos permitir√° codificar y que este dentro del formato aceptado por MongoDB, luego llamamos a la funci√≥n y le enviamos como par√°metro el dataframe producto de la consulta SQL, este m√©todo se puede utilizar tambien para PhpMyadmin_

![image](https://user-images.githubusercontent.com/75056800/156863207-af40a73c-5d9f-4566-998a-707bba9e290f.png)

_Y asi podemos observar que el proceso se a realizado con exito_

![image](https://user-images.githubusercontent.com/75056800/156863540-48fe9a2a-78c5-4a40-bf08-279041a100fd.png)

_Como paso final en la parde de los SGBD los datos que estan concentrados en MongoDB los trataremos con un ultimo script que con la informaci√≥n extraida de Twitter la separarar en dos dataframe como usuarios y tweets._

![image](https://user-images.githubusercontent.com/75056800/156863717-1565ca98-8838-401f-99fa-6d618b675a95.png)

_Creamos nuestra conexi√≥n a MongoDB para extraer los datos y mediante la creaci√≥n de una lista y un diccionario iniciaremos con la extracci√≥n de datos._

![image](https://user-images.githubusercontent.com/75056800/156863764-0eb99286-ce51-41a9-801f-7353b48fe6c8.png)


_Si el proceso es exitoso nos entregara 2 dataframes que ahora podemos convertirlos en documentos sea json o csv para su importaci√≥n en PowerBI._


![image](https://user-images.githubusercontent.com/75056800/156863841-5f48dbdb-5c8c-449d-a533-9c99620f92c2.png)

_El resultado de este proceso sera el siguiente_

![image](https://user-images.githubusercontent.com/75056800/156863917-a2d48075-b345-4ff9-8a20-67d4e38f1f32.png)

_Con esto documentos listos podremos realizar nuestras visualizaci√≥n y extraer informaci√≥n, abrimos nuestro PowerBi y vamos a realizar importaciones de dos maneras una apuntando al archivo csv y otra con el conector a mongo db. _










































